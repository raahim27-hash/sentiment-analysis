# -*- coding: utf-8 -*-
"""Custom Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z5Wq7ph0ffF_1IbGPokuCYNeOruiPuPC

AI PROJECT- SOCIAL MEDIA DATA SENTIMENT ANALYSIS

TRAINING DATASET

AFRAZ ALAM 20K-1874

RAAHIM MUZAFFAR ISHTIAQ 21K-4617
"""

import numpy as np
import re
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('/content/sample_data/Aifinalupd.csv', header=None, encoding='latin')
df.columns = ['label', 'id', 'date', 'query', 'user', 'tweet']


df = df.drop(['id', 'date', 'query', 'user'], axis=1)

import random

possible_labels = ['Positive', 'Negative']


df['label'] = [random.choice(possible_labels) for _ in range(len(df))]

print(df)

label_counts = df.label.value_counts()


plt.figure(figsize=(10,6))
plt.bar(label_counts.index, label_counts.values, color='black')
plt.title("Distribution of Labels")
plt.xlabel("Labels")
plt.ylabel("Count")
plt.show()

"""## Preprocess"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

stop_words = stopwords.words('english')
stemmer = SnowballStemmer('english')

punctuations_and_dummies = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

def preprocess(tweet, will_be_stemmed=False):
        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()
        tokens = []
        for token in tweet.split():
            if token not in stop_words:
                if will_be_stemmed:
                    tokens.append(stemmer.stem(token))
                else:
                    tokens.append(token)
        return " ".join(tokens)

df.tweet = df.tweet.apply(lambda tw: preprocess(tw))

#O WORDS HATA RAHE
df = df[df.iloc[:,1].astype(str).str.len()!=0]

tweets_len = [len(x) for x in df['tweet']]
pd.Series(tweets_len).hist()
plt.show()
pd.Series(tweets_len).describe()
#LENGTH

"""### Number of Letters"""

all_str = ""
for i in df.tweet:
    all_str += i

from collections import Counter

letter_list = ''.join(df['tweet']).lower()


my_counter = Counter(letter_list)


letter_df = pd.DataFrame.from_dict(my_counter, orient='index').reset_index()
letter_df = letter_df.rename(columns={'index':'Letter', 0:'Frequency'})


letter_df = letter_df[letter_df['Letter'].str.isalpha()]


letter_df['Relative Frequency'] = letter_df['Frequency'] / letter_df['Frequency'].sum()


print(letter_df)

"""#### Compare the Observed Frequencies with the Expected Frequencies in English"""

from scipy.stats import chi2_contingency

c, p, dof, expected = chi2_contingency(letter_df[['Frequency', 'Relative Frequency']])
print("p-value:", p)

"""We get that the p-value (p) is 0 which implies that the letter frequency does not follow the same distribution with what we see in English tests, although the Pearson correlation is too high (~96.7%).
MATLAB NOT NULL
"""

df1 = df.copy()

df1['number_of_characters'] = [len(tw) for tw in df1.tweet]
df1

df1.number_of_characters.max()

df1.number_of_characters.min()

df1.number_of_characters.mean()

df1.number_of_characters.std()

"""## Number of Words"""

df1['number_of_words'] = [len(tw.split()) for tw in df1.tweet]
df1

df1.number_of_words.max()

df1.number_of_words.min()
#NULLL AB NAHI HAI KAHON

df1.number_of_words.mean()

df1.number_of_words.std()

"""### Positives + Negatives"""

import collections
from wordcloud import WordCloud
from nltk import word_tokenize, sent_tokenize
from nltk.util import ngrams

all_tweets = ' '.join(df['tweet'].str.lower())

f_words = [word for word in all_tweets.split()]
counted_words = collections.Counter(f_words)

words = []
counts = []
for word, count in counted_words.most_common(20):
    words.append(word)
    counts.append(count)

plt.figure(figsize=(12, 6))
plt.barh(words, counts, color='green')
plt.title('Most Common Words in Tweets')
plt.xlabel('Count')
plt.ylabel('Words')
plt.show()

"""### Positives"""

import collections
import matplotlib.pyplot as plt


all_positive_tweets = ' '.join(df[df.label == 'Positive'].tweet.str.lower())


positive_words = [word for word in all_positive_tweets.split()]


counted_positive_words = collections.Counter(positive_words)


words = []
counts = []
for word, count in counted_positive_words.most_common(20):
    words.append(word)
    counts.append(count)


plt.figure(figsize=(12, 6))
plt.bar(words, counts, color='lightgreen')
plt.title('Most Common Words in Positive Tweets')
plt.xlabel('Words')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

plt.figure(figsize=(25, 25))
plt.axis('off')

wordcloud_fig = WordCloud(max_words=2000, width=1600, height=800, background_color='white', min_font_size=10).generate(" ".join(df[df.label == 'Positive'].tweet))
plt.imshow(wordcloud_fig, interpolation='bilinear')
plt.show()

"""### Negatives"""

all_tweets = ' '.join(df[df.label == 'Negative'].tweet.str.lower())

f_words = [word for word in all_tweets.split()]
counted_words = collections.Counter(f_words)

words = []
counts = []
for letter, count in counted_words.most_common(20):
    words.append(letter)
    counts.append(count)

plt.figure(figsize = (16, 4))
plt.title('Most common words in negative tweets')
plt.xlabel('Count')
plt.ylabel('Words')
plt.bar(words, counts)

from wordcloud import WordCloud

plt.figure(figsize = (25, 25))
plt.axis('off')
wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(" ".join(df[df.label == 'Negative'].tweet))
plt.imshow(wordcloud_fig, interpolation = 'bilinear')

"""### Training Data and Test Data Splitting"""

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(df, test_size=0.2, random_state=7)
print('Training Data', len(train_data), 'Test Data', len(test_data))

train_data.head(10)

test_data.head(10)

"""### Tokenization"""

from keras.preprocessing.text import Tokenizer
#TOKNN TESTDATA

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.tweet)
word_index = tokenizer.word_index
vocab_size = len(tokenizer.word_index) + 1
print("Vocabulary Size :", vocab_size)

"""### GLOVE Embedding"""

MODELS_PATH = 'models'
EMBEDDING_DIMENSION = 300

import tensorflow as tf

BATCH_SIZE = 1024
EPOCHS = 10
LR = 1e-3
#WORDS MAPPING CHECKINGFORPOSTIVIE AND NGATIVE
embeddings_index = {}

glove_file = open('glove/glove.6B.300d.txt', encoding='utf8')
for line in glove_file:
    values = line.split()
    word = value = values[0]
    coefficients = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefficients
glove_file.close()

print('Loaded %s word vectors.' % len(embeddings_index))

embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIMENSION))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_layer = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIMENSION, weights=[embedding_matrix], input_length=30, trainable=False)